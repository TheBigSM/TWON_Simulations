{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code we are calculating the sentiments of posts and plotting them. After that we are checking the names and and posts of the local extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pymongo import MongoClient\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import plotly.io as pio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version of the code with sorted out posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Plotly renderer\n",
    "pio.renderers.default = 'browser'\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(os.getenv('db_url_for_creating'))\n",
    "db = client[os.getenv('db_name')]  # Replace with your database name\n",
    "\n",
    "# Load posts collection\n",
    "posts = pd.DataFrame(list(db.posts.find()))\n",
    "users = pd.DataFrame(list(db.users.find()))\n",
    "\n",
    "# Define function to filter out unwanted posts\n",
    "def clean_posts(post):\n",
    "    if len(post) < 15:  # Remove posts shorter than 15 characters\n",
    "        return None\n",
    "    cleaned_post = post.replace('-', '')  # Replace all '-' symbols with an empty string\n",
    "    if cleaned_post.strip() == '':  # Check if the post is empty after removing '-'\n",
    "        return None\n",
    "    return cleaned_post\n",
    "\n",
    "# Apply the filter function to remove unwanted posts\n",
    "posts['cleaned_desc'] = posts['desc'].apply(clean_posts)\n",
    "posts = posts.dropna(subset=['cleaned_desc'])  # Remove rows with None in 'cleaned_desc' column\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score['compound']  # Return the compound score\n",
    "\n",
    "# Analyze sentiment for each cleaned post\n",
    "posts['sentiment'] = posts['cleaned_desc'].apply(analyze_sentiment)\n",
    "\n",
    "# Reset index to get a sequential label for filtered posts\n",
    "posts = posts.reset_index(drop=True)\n",
    "posts['post_label'] = 'post ' + (posts.index + 1).astype(str)\n",
    "\n",
    "# Convert ObjectId to string\n",
    "posts['userId'] = posts['userId'].astype(str)\n",
    "\n",
    "# Simplify user IDs for filtered posts\n",
    "user_mapping = {user_id: f'User {i + 1}' for i, user_id in enumerate(posts['userId'].unique())}\n",
    "posts['user_label'] = posts['userId'].map(user_mapping)\n",
    "\n",
    "# Save mapped posts and user labels if needed for future reference\n",
    "# Example of saving to a CSV or other file format if needed:\n",
    "posts[['post_label', 'cleaned_desc', 'sentiment', 'user_label']].to_csv('filtered_posts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting for users by user no.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure\n",
    "fig = make_subplots()\n",
    "\n",
    "# Plot each user's posts\n",
    "for user_id, user_label in user_mapping.items():\n",
    "    user_posts = posts[posts['userId'] == user_id]\n",
    "    \n",
    "    # Plot only the user's posts as a line connecting their dots\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=user_posts.index + 1,    # Plot the post number (linear sequence)\n",
    "        y=user_posts['sentiment'],  # Plot the sentiment score\n",
    "        mode='lines+markers',          # Lines and markers\n",
    "        name=user_label,               # User label for legend\n",
    "        line=dict(width=1),            # Thin line\n",
    "        marker=dict(size=8)            # Adjust marker size\n",
    "    ))\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    title='Sentiment of posts per User',\n",
    "    xaxis_title='posts (Sequential)',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',            # Keep linear tick mode\n",
    "        tickvals=list(range(1, len(posts) + 1)),  # Ticks for each post\n",
    "        ticktext=[str(i) for i in range(1, len(posts) + 1)],  # Custom tick text\n",
    "        range=[1, len(posts)],     # Set x-axis range to start at 1\n",
    "        showgrid=True,                # Optionally show grid lines for better readability\n",
    "        zeroline=True                 # Optionally show a line at y=0\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        autorange=True,               # Automatically adjust y-axis range\n",
    "    ),\n",
    "    legend_title=\"Users\",\n",
    "    hovermode=\"closest\",\n",
    "    width=1600,                    # Increase chart width for clarity\n",
    "    height=600,\n",
    "    margin=dict(l=40, r=40, t=50, b=40)  # Add margins to avoid clutter\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding specific user's unfo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post Number 1 :\n",
      "Disappointed to see Trump attend a civil rights museum opening without acknowledging the ongoing struggle for racial justice. Black leaders staying away sends a powerful message about the need for genuine commitment to equality.\n"
     ]
    }
   ],
   "source": [
    "# Find specific post details\n",
    "# post (xy) by User (vw)\n",
    "specific_post = posts[(posts['post_label'] == 'post 1') & (posts['user_label'] == 'User 1')]\n",
    "\n",
    "if not specific_post.empty:\n",
    "    post_text = specific_post['desc'].values[0]\n",
    "    print(f\"post Number 1 :\\n{post_text}\")\n",
    "else:\n",
    "    print(\"post not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append(r'C:\\Users\\matej\\AppData\\Roaming\\nltk_data')\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Set up stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the post text for topic modeling\n",
    "def preprocess_text(text):\n",
    "    # Lowercase, remove special characters, and tokenize\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and short words\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    return words\n",
    "\n",
    "# Apply preprocessing to each post\n",
    "posts['processed_desc'] = posts['cleaned_desc'].apply(preprocess_text)\n",
    "\n",
    "# Create a dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(posts['processed_desc'])\n",
    "corpus = [dictionary.doc2bow(text) for text in posts['processed_desc']]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5  # Define the number of topics\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Display the main topics\n",
    "#for idx, topic in lda_model.print_topics(-1):\n",
    "  #  print(f\"Topic {idx + 1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2275271400551480646565267450592\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2275271400551480646565267450592_data = {\"mdsDat\": {\"x\": [0.1588023380682547, 0.1641910110470699, -0.2503185124816767, -0.038173612843808914, -0.034501223789839164], \"y\": [-0.18677009350720483, 0.17516322155626657, -0.006126958600774433, 0.006140749672802546, 0.011593080878910028], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [34.02673960904983, 33.950674688649976, 28.478695093452394, 1.772009435668902, 1.7718811731788975]}, \"tinfo\": {\"Term\": [\"justice\", \"must\", \"tarnishing\", \"equality\", \"stand\", \"hold\", \"damaging\", \"need\", \"accountable\", \"restore\", \"values\", \"country\", \"harming\", \"shows\", \"evidencebased\", \"dialogue\", \"understanding\", \"let\", \"mutual\", \"constructive\", \"promote\", \"analysis\", \"reputation\", \"trump\", \"actions\", \"stand\", \"equality\", \"tarnishing\", \"must\", \"justice\", \"country\", \"trump\", \"actions\", \"reputation\", \"analysis\", \"constructive\", \"promote\", \"understanding\", \"mutual\", \"let\", \"dialogue\", \"evidencebased\", \"shows\", \"harming\", \"values\", \"damaging\", \"need\", \"hold\", \"restore\", \"accountable\", \"restore\", \"accountable\", \"need\", \"damaging\", \"values\", \"hold\", \"country\", \"actions\", \"trump\", \"reputation\", \"constructive\", \"evidencebased\", \"shows\", \"harming\", \"dialogue\", \"let\", \"analysis\", \"understanding\", \"promote\", \"mutual\", \"tarnishing\", \"equality\", \"stand\", \"must\", \"justice\", \"analysis\", \"promote\", \"mutual\", \"constructive\", \"let\", \"understanding\", \"dialogue\", \"evidencebased\", \"shows\", \"harming\", \"trump\", \"actions\", \"reputation\", \"values\", \"hold\", \"damaging\", \"need\", \"accountable\", \"restore\", \"equality\", \"stand\", \"tarnishing\", \"must\", \"justice\", \"country\", \"harming\", \"shows\", \"evidencebased\", \"dialogue\", \"understanding\", \"let\", \"mutual\", \"constructive\", \"promote\", \"analysis\", \"values\", \"hold\", \"damaging\", \"need\", \"restore\", \"accountable\", \"justice\", \"must\", \"tarnishing\", \"equality\", \"stand\", \"country\", \"reputation\", \"trump\", \"actions\", \"analysis\", \"constructive\", \"promote\", \"mutual\", \"let\", \"dialogue\", \"understanding\", \"evidencebased\", \"shows\", \"harming\", \"hold\", \"damaging\", \"need\", \"accountable\", \"restore\", \"values\", \"equality\", \"stand\", \"tarnishing\", \"must\", \"justice\", \"country\", \"trump\", \"actions\", \"reputation\"], \"Freq\": [17.0, 17.0, 17.0, 17.0, 17.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 32.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 42.0, 42.0, 42.0, 16.762086225813754, 16.761894859562403, 16.76190631861937, 16.76192121539343, 16.761583173212898, 16.761908610430762, 16.76028829977561, 16.760268819378766, 16.75977951764627, 0.18432152750889283, 0.18432396255849834, 0.1843231031292258, 0.18432589627436155, 0.18431982655512427, 0.184321813985317, 0.18432236903338886, 0.1843230852244493, 0.18432274503369558, 0.18432235112861234, 0.18433574390144267, 0.18433504561515873, 0.1843335595187083, 0.18433196599359883, 0.18432831341919056, 0.18432802694276637, 15.063761861890946, 15.063669251019897, 15.063582356869283, 15.06355262992302, 15.0635103261918, 15.063465735772406, 15.066669385904268, 15.066070273602664, 15.065971946011178, 15.065803874430385, 0.18665158320373534, 0.1866023122194848, 0.18657889153045204, 0.1865636707622934, 0.1865399463724875, 0.18650110840303114, 0.18647104202649248, 0.18646507519953828, 0.1864541241069547, 0.18644281571934387, 0.1891764263781773, 0.18915282704163092, 0.18915068327146772, 0.18914409117821593, 0.18908831742447044, 9.725324446131474, 9.72488519377032, 9.724869848709668, 9.724659813191996, 9.724538011773074, 9.724316467459914, 9.724195625107283, 9.724051765163672, 9.724009566246881, 9.723998057451393, 9.727475631821585, 9.727410415313814, 9.726147325008922, 0.19165250964474967, 0.19159481581319784, 0.19154110810091687, 0.19151952910937542, 0.19147219019668135, 0.19139374157118166, 0.1914638583082806, 0.19137616368432184, 0.19131487335417977, 0.19129758019012502, 0.19126145036470388, 0.19193138813960148, 0.3263462830188843, 0.3263007806068299, 0.32619181351907733, 0.3260634221229658, 0.3259937810542871, 0.3255860197669852, 0.32515400047247, 0.32513529226764504, 0.3251036643615613, 0.3243769684628171, 0.3054768478907234, 0.3054671506553675, 0.30546333143651966, 0.30546183955415723, 0.3054594227047301, 0.3054584679000181, 0.3176129230197533, 0.31690460711171436, 0.3168395908783592, 0.3166475259430188, 0.3164816584619628, 0.31690392084582764, 0.3418595322517682, 0.33860531909222646, 0.33858798341917495, 0.3179658008711408, 0.3179714099427955, 0.31796708379710437, 0.3179643687677396, 0.3179658307066283, 0.3179704552071947, 0.317964279261277, 0.3179663080744287, 0.3179669942906418, 0.3179647267935899, 0.323773576872757, 0.32370990794237864, 0.32370182252525936, 0.32364498592152424, 0.3236158068147248, 0.3235827789300346, 0.31807123948405414, 0.31806398946058556, 0.31806076722793286, 0.318051965759113, 0.3180535470399518, 0.32407610871626047, 0.32428257028993346, 0.3242734107952633, 0.3242725754016126], \"Total\": [17.0, 17.0, 17.0, 17.0, 17.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 32.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 42.0, 42.0, 42.0, 17.777158720692093, 17.77723031033939, 17.777297976458023, 17.777319459632597, 17.777599411061775, 32.661489414036716, 42.21662376699053, 42.21661090250968, 42.21786282473896, 10.738459785000817, 10.73874206116467, 10.738733169165165, 10.739065499249378, 10.738750860224345, 10.738912784635035, 10.73909181784332, 10.739135284201112, 10.7391789777085, 10.739195089154773, 16.06855820655875, 16.068602023017995, 16.068599107576784, 16.06863324510733, 16.068559146400773, 16.068572921980888, 16.068559146400773, 16.068572921980888, 16.068599107576784, 16.068602023017995, 16.06855820655875, 16.06863324510733, 32.661489414036716, 42.21661090250968, 42.21662376699053, 42.21786282473896, 10.73874206116467, 10.739135284201112, 10.7391789777085, 10.739195089154773, 10.73909181784332, 10.738912784635035, 10.738459785000817, 10.739065499249378, 10.738733169165165, 10.738750860224345, 17.777297976458023, 17.77723031033939, 17.777158720692093, 17.777319459632597, 17.777599411061775, 10.738459785000817, 10.738733169165165, 10.738750860224345, 10.73874206116467, 10.738912784635035, 10.739065499249378, 10.73909181784332, 10.739135284201112, 10.7391789777085, 10.739195089154773, 42.21662376699053, 42.21661090250968, 42.21786282473896, 16.06855820655875, 16.06863324510733, 16.068602023017995, 16.068599107576784, 16.068572921980888, 16.068559146400773, 17.77723031033939, 17.777158720692093, 17.777297976458023, 17.777319459632597, 17.777599411061775, 32.661489414036716, 10.739195089154773, 10.7391789777085, 10.739135284201112, 10.73909181784332, 10.739065499249378, 10.738912784635035, 10.738750860224345, 10.73874206116467, 10.738733169165165, 10.738459785000817, 16.06855820655875, 16.06863324510733, 16.068602023017995, 16.068599107576784, 16.068559146400773, 16.068572921980888, 17.777599411061775, 17.777319459632597, 17.777297976458023, 17.77723031033939, 17.777158720692093, 32.661489414036716, 42.21786282473896, 42.21662376699053, 42.21661090250968, 10.738459785000817, 10.73874206116467, 10.738733169165165, 10.738750860224345, 10.738912784635035, 10.73909181784332, 10.739065499249378, 10.739135284201112, 10.7391789777085, 10.739195089154773, 16.06863324510733, 16.068602023017995, 16.068599107576784, 16.068572921980888, 16.068559146400773, 16.06855820655875, 17.77723031033939, 17.777158720692093, 17.777297976458023, 17.777319459632597, 17.777599411061775, 32.661489414036716, 42.21662376699053, 42.21661090250968, 42.21786282473896], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.2165, -2.2166, -2.2165, -2.2165, -2.2166, -2.2165, -2.2166, -2.2166, -2.2167, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -6.7267, -2.3211, -2.3211, -2.3211, -2.3211, -2.3211, -2.3211, -2.3209, -2.321, -2.321, -2.321, -6.7119, -6.7122, -6.7123, -6.7124, -6.7125, -6.7127, -6.7129, -6.7129, -6.713, -6.7131, -6.6985, -6.6986, -6.6986, -6.6987, -6.699, -2.5829, -2.583, -2.583, -2.583, -2.583, -2.583, -2.5831, -2.5831, -2.5831, -2.5831, -2.5827, -2.5827, -2.5829, -6.5097, -6.51, -6.5103, -6.5104, -6.5107, -6.5111, -6.5107, -6.5112, -6.5115, -6.5116, -6.5118, -6.5083, -3.2004, -3.2006, -3.2009, -3.2013, -3.2015, -3.2028, -3.2041, -3.2041, -3.2042, -3.2065, -3.2665, -3.2665, -3.2666, -3.2666, -3.2666, -3.2666, -3.2275, -3.2298, -3.23, -3.2306, -3.2311, -3.2298, -3.154, -3.1635, -3.1636, -3.2264, -3.2263, -3.2264, -3.2264, -3.2264, -3.2264, -3.2264, -3.2264, -3.2264, -3.2264, -3.2083, -3.2085, -3.2085, -3.2087, -3.2088, -3.2089, -3.226, -3.2261, -3.2261, -3.2261, -3.2261, -3.2073, -3.2067, -3.2067, -3.2067], \"loglift\": [25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0192, 1.0192, 1.0192, 1.0192, 1.0192, 0.4109, 0.1542, 0.1542, 0.1542, -2.9869, -2.9869, -2.9869, -2.9869, -2.9869, -2.9869, -2.9869, -2.9869, -2.9869, -2.9869, -3.3898, -3.3898, -3.3899, -3.3899, -3.3899, -3.3899, 1.0157, 1.0157, 1.0157, 1.0157, 1.0157, 1.0157, 0.3065, 0.0499, 0.0499, 0.0498, -2.9721, -2.9724, -2.9725, -2.9726, -2.9727, -2.9729, -2.973, -2.9731, -2.9732, -2.9732, -3.4627, -3.4629, -3.4629, -3.4629, -3.4632, 1.1569, 1.1568, 1.1568, 1.1568, 1.1568, 1.1568, 1.1567, 1.1567, 1.1567, 1.1567, -0.2118, -0.2119, -0.212, -3.1729, -3.1732, -3.1735, -3.1736, -3.1739, -3.1743, -3.275, -3.2754, -3.2757, -3.2758, -3.276, -3.8808, 0.5394, 0.5392, 0.5389, 0.5385, 0.5383, 0.5371, 0.5357, 0.5357, 0.5356, 0.5334, 0.0703, 0.0703, 0.0703, 0.0703, 0.0703, 0.0702, 0.0082, 0.006, 0.0058, 0.0052, 0.0047, -0.6023, -0.7831, -0.7927, -0.7927, 0.5135, 0.5135, 0.5135, 0.5135, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.5134, 0.1285, 0.1284, 0.1283, 0.1282, 0.1281, 0.128, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, -0.5798, -0.8358, -0.8359, -0.8359]}, \"token.table\": {\"Topic\": [2, 1, 2, 3, 3, 3, 1, 2, 2, 3, 1, 3, 3, 2, 1, 3, 1, 3, 2, 3, 1, 2, 3, 2, 3, 1, 1, 1, 2, 3, 3, 2], \"Freq\": [0.9334992020032382, 0.40268509566667726, 0.3553103785294211, 0.23687358568628075, 0.9312322437494922, 0.9312077655877182, 0.520490654437027, 0.4592564597973768, 0.9334975113897749, 0.9311774374984579, 0.9562794486671331, 0.9311736685831222, 0.9311684830177575, 0.9334956975614145, 0.9562595942746955, 0.9311929615731442, 0.9562746531389237, 0.9312070025797291, 0.9334976807609252, 0.9312085366562288, 0.40267315450270225, 0.3552998422082667, 0.2368665614721778, 0.9335000022923573, 0.9311698800026681, 0.9562832996598324, 0.956275808759724, 0.40268497295826905, 0.3553102702572962, 0.23687351350486416, 0.9311797195667504, 0.9335000568923109], \"Term\": [\"accountable\", \"actions\", \"actions\", \"actions\", \"analysis\", \"constructive\", \"country\", \"country\", \"damaging\", \"dialogue\", \"equality\", \"evidencebased\", \"harming\", \"hold\", \"justice\", \"let\", \"must\", \"mutual\", \"need\", \"promote\", \"reputation\", \"reputation\", \"reputation\", \"restore\", \"shows\", \"stand\", \"tarnishing\", \"trump\", \"trump\", \"trump\", \"understanding\", \"values\"]}, \"R\": 25, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 5, 2, 4, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2275271400551480646565267450592\", ldavis_el2275271400551480646565267450592_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2275271400551480646565267450592\", ldavis_el2275271400551480646565267450592_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2275271400551480646565267450592\", ldavis_el2275271400551480646565267450592_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_vis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the sentiments per post for users with their names attached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique userIds in posts: ['6735be3221442c54636b4bc0' '6735be3221442c54636b4bc6'\n",
      " '6735be3321442c54636b4bd1' '6735be3221442c54636b4bbf'\n",
      " '6735be3221442c54636b4bc3' '6735be3221442c54636b4bc7'\n",
      " '6735be3221442c54636b4bcd' '6735be3221442c54636b4bbe'\n",
      " '6735be3221442c54636b4bc1' '6735be3221442c54636b4bcf'\n",
      " '6735be3221442c54636b4bc5' '6735be3221442c54636b4bc9'\n",
      " '6735be3221442c54636b4bcc']\n",
      "Unique _id in users: ['6735be3221442c54636b4bbe' '6735be3221442c54636b4bbf'\n",
      " '6735be3221442c54636b4bc0' '6735be3221442c54636b4bc1'\n",
      " '6735be3221442c54636b4bc2' '6735be3221442c54636b4bc3'\n",
      " '6735be3221442c54636b4bc4' '6735be3221442c54636b4bc5'\n",
      " '6735be3221442c54636b4bc6' '6735be3221442c54636b4bc7'\n",
      " '6735be3221442c54636b4bc8' '6735be3221442c54636b4bc9'\n",
      " '6735be3221442c54636b4bca' '6735be3221442c54636b4bcb'\n",
      " '6735be3221442c54636b4bcc' '6735be3221442c54636b4bcd'\n",
      " '6735be3221442c54636b4bce' '6735be3221442c54636b4bcf'\n",
      " '6735be3321442c54636b4bd0' '6735be3321442c54636b4bd1']\n",
      "Number of NaN usernames after merge: 0\n"
     ]
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Ensure the columns are of the same type and strip whitespace\n",
    "posts['userId'] = posts['userId'].astype(str).str.strip()\n",
    "users['_id'] = users['_id'].astype(str).str.strip()\n",
    "\n",
    "# Print unique values to check for mismatches\n",
    "print(\"Unique userIds in posts:\", posts['userId'].unique())\n",
    "print(\"Unique _id in users:\", users['_id'].unique())\n",
    "\n",
    "# Merge posts with users to get usernames using the correct column name\n",
    "merged_data = posts.merge(users[['_id', 'username']], left_on='userId', right_on='_id', how='left')\n",
    "\n",
    "# Check for NaN values in merged usernames\n",
    "print(\"Number of NaN usernames after merge:\", merged_data['username'].isna().sum())\n",
    "\n",
    "# Create a figure\n",
    "fig = make_subplots()\n",
    "\n",
    "# Plot each user's posts using actual usernames from the merged DataFrame\n",
    "for user_id in merged_data['userId'].unique():\n",
    "    username = merged_data.loc[merged_data['userId'] == user_id, 'username'].iloc[0]\n",
    "    user_posts = merged_data[merged_data['userId'] == user_id]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=user_posts.index + 1,\n",
    "        y=user_posts['sentiment'],\n",
    "        mode='lines+markers',\n",
    "        name=username,\n",
    "        line=dict(width=1),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    title='Sentiment of Posts per User',\n",
    "    xaxis_title='Posts (Sequential)',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',\n",
    "        tickvals=list(range(1, len(merged_data) + 1)),\n",
    "        ticktext=[str(i) for i in range(1, len(merged_data) + 1)],\n",
    "        range=[1, len(merged_data)],\n",
    "        showgrid=True,\n",
    "        zeroline=True\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        autorange=True,\n",
    "    ),\n",
    "    legend_title=\"Users\",\n",
    "    hovermode=\"closest\",\n",
    "    width=1600,\n",
    "    height=600,\n",
    "    margin=dict(l=40, r=40, t=50, b=40)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting the names to roles and than graphing by the roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['academic communications', 'academic researcher', 'academic researcher', 'academic researcher', 'academic researcher', 'academic science', 'academic science', 'academic science', 'academic writer', 'biologist', 'biology', 'biotechnology researcher', 'climate science', 'editor', 'editor', 'editor', 'educator', 'entry-level biology', 'environmental science', 'environmental', 'environmental', 'general-interest science', 'journalist', 'journalist', 'medical', 'microbiologist', 'microbiologist', 'molecular biologist', 'news aggregator', 'research coordinator', 'science communications', 'science communicator,', 'science communicator', 'science communicator', 'science communicator', 'science communicator', 'science editor', 'science editor', 'science editor', 'science editor', 'science enthusiast', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science journalist', 'science news', 'science news', 'science news', 'science news', 'science news', 'science news', 'science news', 'science news', 'science news', 'science news', 'science news', 'science news', 'science reporter', 'science researcher', 'science researcher', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science writer', 'science technology', 'science', 'science', 'science', 'science', 'science', 'science', 'science', 'science', 'science', 'science', 'scientific editor', 'scientific journalist', 'scientific journalist', 'scientific research', 'scientific researcher', 'scientist']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Function to extract roles from the text file\n",
    "def extract_roles_from_file(file_path):\n",
    "    roles = []\n",
    "    \n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            # Use regex to find the persona value\n",
    "            match = re.search(r\"persona:\\s*'([^']+)'\", line)\n",
    "            if match:\n",
    "                roles.append(match.group(1))  # Append the extracted persona to the list\n",
    "\n",
    "    return roles\n",
    "\n",
    "# Specify the path to your text file\n",
    "file_path = 'roles_with_agents.txt'\n",
    "\n",
    "# Get the roles\n",
    "roles_array = extract_roles_from_file(file_path)\n",
    "\n",
    "# Print the roles array\n",
    "print(roles_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dunkelschieferblaue Biene', 'Sienna-Kuh', 'Darkslategray-Wissenschaftler', 'Dunkelschiefergraue Katze', 'Silberner BÃ¤r', 'SeegrÃ¼ner Papagei', 'Kadettenblaues Siegel', 'Dunkelschieferblaue Giraffe', 'Hellschiefergrauer LÃ¶we', 'Dunkelkhakifarbenes Rentier', 'Siena Biene', 'Peru-BÃ¼ffel', 'Peruanisches ErdmÃ¤nnchen', 'DÃ¼sterer Professor', 'Rosabraunes Zebra', 'Dunkelschiefergraue Kuh', 'Grauer Koala', 'Dunkelgrauer Orca', 'Darkslategray-Wissenschaftler', 'Silberner BlauhÃ¤her', 'Graues Huhn', 'Dunkelschiefergrauer Frosch', 'Silberner WaschbÃ¤r', 'Rosabraunes Lamm', 'BlaugrÃ¼ne Blume', 'Schwarzer Biker', 'Dunkelschiefergrauer Fuchs', 'Dunkelgrauer Igel', 'Schiefergraue Eule', 'Dunkelschiefergrauer Hund', 'Rosabrauner Fuchs', 'Himmelblauer Delphin', 'Dunkellachs-Warzenschwein', 'Rosabrauner Vogel', 'Dunkelschiefergraues Schwein', 'Schwarzer Tiger', 'Indigo-Truthahn', 'Schwarzer Astronaut', 'Nachtblauer Cowboy', 'Hellstahlblaue Krabbe', 'Dunkelschiefergraues Siegel', 'Dunkelschiefergrauer Wolf', 'Indianerhund', 'Schwarzer AuÃŸerirdischer', 'Schwarzer LÃ¶we', 'Dunkelkhakifarbenes ErdmÃ¤nnchen', 'Silberner Adler', 'Dunkelschieferblauer Papagei', 'MitteltÃ¼rkisfarbener Wal', 'DunkelseegrÃ¼ner Biber', 'Peru Cowboy', 'Siena Gans', 'Schwarzes Krokodil', 'Dunkelschieferblauer Pinguin', 'Dunkelgrauer Esel', 'Dunkelschiefergrauer Alien', 'Dunkelschiefergrauer Igel', 'Peruanisches Frettchen', 'Dunkelschiefergraues Kaninchen', 'Dunkelgraues EichhÃ¶rnchen', 'Dunkelschieferblaue Gans', 'Dunkelschiefergrauer Wolf', 'Stahlblaue Giraffe', 'Sienna-Hai', 'Graue Biene', 'Hellgrauer Professor', 'Darkslategray-Wissenschaftler', 'Rosabraunes Walross', 'Stahlblauer Bison', 'Kadettenblauer Wolf', 'Dunkelgraue Kuh', 'Dunkelschiefergraues Kaninchen', 'Dunkelgrauer Koala', 'Dunkelgrauer Papagei', 'MittelgroÃŸer Aquamarin-Truthahn', 'Gainsboro-Faultier', 'Stahlblaues Nilpferd', 'Burlywood-Leopard', 'Graue Eule', 'Hellstahlblaue MÃ¶we', 'Blassviolettes Warzenschwein', 'Kadettenblaues Huhn', 'Dunkelschiefergrauer Frosch', 'Stahlblaufuchs', 'Hellstahlblaues EichhÃ¶rnchen', 'Dunkelschiefergraues Faultier', 'Dunkelschieferblauer Otter', 'Dunkelschiefergrauer Igel', 'Dunkelgraue Kuh', 'Weizenvogel', 'Rosigbraunes Rotkehlchen', 'Dunkelschieferblaue Blume', 'Silberner Hund', 'Dunkelgraues Zebra', 'DunkelseegrÃ¼nes Zebra', 'Kadettenblauer Oktopus', 'Burlywood-Adler', 'Dunkelgraue Seekuh', 'Grauer Panda', 'Dunkelschieferblaue Krabbe', 'Schwarzer Frosch', 'Peruanisches ErdmÃ¤nnchen', 'Kadettenblauer Adler', 'Dunkelgrauer WaschbÃ¤r', 'MitteltÃ¼rkiser Hai', 'Schokoladen-EichhÃ¶rnchen', 'Dunkelgraues Frettchen', 'Dunkelgrauer Pinguin', 'Dunkelgrauer Cowboy', 'Silberner Cowboy', 'Dunkelschiefergrauer Hund', 'Silberne Feuerwehrfrau', 'Dunkelschiefergrauer Astronaut', 'Dunkelgrauer Biker', 'BlaugrÃ¼ner BÃ¤r', 'Schwarze Ziege', 'Weizenflusspferd', 'Kadettenblaue Katze', 'Rosabraunes Huhn', 'Dunkelschiefergraues Krokodil', 'Kadettenblaues Schwein', 'Dunkelschiefergrauer Hirsch', 'Dunkelschiefergrauer Elefant', 'Dunkelgraues Pferd', 'Dunkelschiefergrauer Tiger', 'Feuerwehrmann Aus Schokolade', 'Indianred-Katze', 'Stahlblaue Giraffe', 'Blaue Giraffe', 'Dunkelschiefergraue Seekuh', 'Dunkelschiefergrauer LÃ¶we', 'Burlywood-Samurai', 'Blauer Wissenschaftler', 'Blaue Seekuh', 'Dunkelschiefergrauer Otter', 'Dunkelgrauer Pinguin', 'Dunkelschiefergrauer Biker', 'Black Samurai', 'Schieferblaue Blume', 'Blaue Kuh', 'Blauer Wal', 'Dunkelgrauer Feuerwehrmann', 'Blauer Esel', 'Blauer Koch', 'Blauer Ritter', 'Blauer Adler', 'Blauer AuÃŸerirdischer', 'Blauer Astronaut', 'Blauer WaschbÃ¤r', 'Schwarze Ziege', 'Indisches Walross', 'Dunkelschieferblauer Hai', 'Rosabraunes Nashorn', 'Blaue Eule', 'Rosabraunes ErdmÃ¤nnchen', 'Schwarzer Tiger', 'Dunkelschiefergrauer Astronaut', 'Anthrazitgraue Ziege', 'Blauer Pinguin', 'Blauer Tiger', 'Blaue Krabbe', 'Blauer Hai', 'Blauer Otter', 'Blauer Schwan', 'Dunkelschiefergrauer Astronaut', 'Blauer Bison', 'Blauer Cowboy', 'Blauer Frosch', 'Blauer Oktopus', 'Dunkelschieferblaue MÃ¶we', 'Dunkelkhakifarbener Truthahn', 'Schiefergraues Schaf', 'Biskuit-Samurai', 'Blaue Blume', 'Blauer Delphin', 'Dunkelschiefergraues Schwein', 'Blauer Hund', 'Dunkelschieferblauer Schwan', 'Lichtschiefergraue Gans', 'Dunkelschiefergraues Kaninchen', 'Blaue Gans', 'Blaue MÃ¶we', 'Blauer Hirsch', 'Serene swan', 'Blauer Soldat', 'Stahlblaue Eule', 'Black Samurai', 'Schwarze Blume', 'Blaue Katze', 'Blaue Schafe', 'Blauer BÃ¤r', 'Blauer Fuchs', 'Friendly Crocodile', 'Blaues Siegel', 'Blauer Wrestler', 'Graue Feuerwehrfrau', 'Gelber Adler', 'Gelbes ErdmÃ¤nnchen', 'Grauer Vogel', 'GrÃ¼ner Cowboy', 'Gelber Vogel', 'Goldene Katze', 'Friendly Wolf', 'GrÃ¼nes Huhn', 'Dunkelgrauer Igel', 'Gelbes Nilpferd', 'Gelber Truthahn', 'Graue Wissenschaftlerin', 'Grauer Astronaut', 'Grauer Professor', 'Graues Zebra', 'GrÃ¼ne Eule', 'GrÃ¼ner Papagei', 'Brauner Soldat', 'Brauner Cowboy', 'Gelber Samurai', 'Blaues Huhn', 'GrÃ¼ne Seekuh', 'Grauer Orca', 'Graue Pansa', 'Grauer Fuchs', 'Grauer Koala', 'Goldener Leopard', 'Goldenes ErdmÃ¤nnchen', 'Grauer Adler', 'Grauer Biker', 'Grauer LÃ¶we', 'Blaues Schwein', 'BlaugrÃ¼ner Panda', 'Goldene Giraffe', 'GrÃ¼ner Pirat', 'GrÃ¼nes ErdmÃ¤nnchen', 'Graues Faultier', 'Graues Kaninchen', 'GrÃ¼ner Biber', 'GrÃ¼ner Pinguin', 'GrÃ¼ner BÃ¤r', 'Graue Kuh', 'Dunkelblauer Otter', 'Brauner Koch', 'Blauer Wolf', 'Friendly Turkey', 'Dunkelblauer Pinguin', 'GrÃ¼ner Koala', 'GrÃ¼ner Wissenschaftler', 'Grauer Cowboy', 'Graues Huhn', 'Graues EichhÃ¶rnchen', 'Graues Frettchen', 'Graues Schwein', 'Graue Professorin', 'Graues Pferd', 'Grauer Papagei', 'GrÃ¼ner Frosch', 'GrÃ¼ner Hai', 'GrÃ¼ner Tiger', 'GrÃ¼nes Siegel', 'Lila Giraffe', 'Lila Samurai']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract names from the text file\n",
    "def extract_names_from_file(file_path):\n",
    "    names = []\n",
    "    \n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            # Use regex to find the name value\n",
    "            match = re.search(r'=\\s*\"([^\"]+)\"', line)\n",
    "            if match:\n",
    "                names.append(match.group(1))  # Append the extracted name to the list\n",
    "\n",
    "    return names\n",
    "\n",
    "# Specify the path to your text file\n",
    "file_path = 'names_with_agents.txt'\n",
    "\n",
    "# Get the names\n",
    "names_array = extract_names_from_file(file_path)\n",
    "\n",
    "# Print the names array\n",
    "print(names_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Map usernames to their roles\n",
    "user_role_mapping = dict(zip(names_array, roles_array))\n",
    "\n",
    "\n",
    "# Map each post's username to its role\n",
    "posts['role'] = posts['username'].map(user_role_mapping)\n",
    "\n",
    "# Create a figure\n",
    "fig = make_subplots()\n",
    "\n",
    "# Plot each role's posts without connecting individual users' posts\n",
    "for role in posts['role'].unique():\n",
    "    # Filter posts for the current role\n",
    "    role_posts = posts[posts['role'] == role]\n",
    "    \n",
    "    # Plot posts for this role as individual points\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=role_posts.index + 1,    # Sequential post order\n",
    "        y=role_posts['sentiment'],  # Sentiment score\n",
    "        mode='lines+markers',          # Lines and markers\n",
    "        name=role,                     # Role label for legend\n",
    "        marker=dict(size=8),            # Adjust marker size\n",
    "        line=dict(width=0.7)            # Thin line\n",
    "    ))\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    title='Sentiment of posts per Role',\n",
    "    xaxis_title='posts (Sequential)',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    xaxis=dict(\n",
    "        tickmode='linear',               # Keep linear tick mode\n",
    "        tickvals=list(range(1, len(posts) + 1)),  # Ticks for each post\n",
    "        ticktext=[str(i) for i in range(1, len(posts) + 1)],  # Custom tick text\n",
    "        range=[1, len(posts)],        # Set x-axis range to start at 1\n",
    "        showgrid=True,                   # Optionally show grid lines for readability\n",
    "        zeroline=True                    # Optionally show a line at y=0\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        autorange=True,                  # Automatically adjust y-axis range\n",
    "    ),\n",
    "    legend_title=\"Roles\",\n",
    "    hovermode=\"closest\",\n",
    "    width=1600,                          # Increase chart width for clarity\n",
    "    height=600,\n",
    "    margin=dict(l=40, r=40, t=50, b=40)  # Add margins to avoid clutter\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
